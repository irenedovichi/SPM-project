# Project 2 - SPM course 23/24
## Overview
In this project, a file compression and decompression program was developed using the Miniz library. A sequential implementation was first created to serve as a baseline reference for performance evaluation. Building upon this, two parallel implementations have been developed:
- A shared-memory version using FastFlow, which utilizes only the pipeline and all-to-all parallel building blocks.
- A distributed-memory version using Open MPI, designed to enable parallelism across multiple nodes in a cluster.

## Project Structure
```
./
â”œâ”€â”€ ðŸ“‚ Distributed
â”‚   â”œâ”€â”€ ðŸ“„ Makefile
â”‚   â”œâ”€â”€ ðŸ“„ cmdline_mpi.hpp
â”‚   â”œâ”€â”€ ðŸ“„ mainmpi.cpp
â”‚   â””â”€â”€ ðŸ“„ utility_mpi.hpp
â”œâ”€â”€ ðŸ“‚ Sequential
â”‚   â”œâ”€â”€ ðŸ“„ Makefile
â”‚   â”œâ”€â”€ ðŸ“„ cmdline_seq.hpp
â”‚   â”œâ”€â”€ ðŸ“„ mainseq.cpp
â”‚   â””â”€â”€ ðŸ“„ utility_seq.hpp
â”œâ”€â”€ ðŸ“‚ SharedMemory
â”‚   â”œâ”€â”€ ðŸ“„ Makefile
â”‚   â”œâ”€â”€ ðŸ“„ cmdline_ff.hpp
â”‚   â”œâ”€â”€ ðŸ“„ mainff.cpp
â”‚   â””â”€â”€ ðŸ“„ utility_ff.hpp
â”œâ”€â”€ ðŸ“‚ miniz 
â”œâ”€â”€ ðŸ“‚ shellscripts
â”œâ”€â”€ ðŸ“‚ src
â”‚   â””â”€â”€ ðŸ“„ file_generator.py
â”œâ”€â”€ ðŸ“„ plots_decomp.ipynb
â””â”€â”€ ðŸ“„ plots.ipynb
```
### Sequential Implementation
To compile the code make sure that you are in the `Sequential` folder and use the provided Makefile:
```
cd Sequential
make mainseq
```

---
### MPI Implementation
To compile the code make sure that you are in the `Distributed` folder and use the provided Makefile:
```
cd Distributed
make mainmpi
```

---

### FastFlow Implementation
In order to use FastFlow, get the `fastflow` directory trough the command: 
```
git clone https://github.com/fastflow/fastflow.git
```
Inside `fastflow` you will have a folder named `ff`, run this script inside it: `mapping_string.sh`.

To compile the code make sure that you are in the `SharedMemory` folder and use the provided Makefile:
```
cd SharedMemory
make mainff
```

## Experiments
The shell scripts that were used to test on the SPM Cluster Machine Backend nodes can be found in the `shellscripts` folder. 

To run one of those script use:
```
sbatch [path-to-script]
```
